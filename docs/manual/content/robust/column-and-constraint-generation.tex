\section{Two-stage robust problems}

\subsection{Problem statement and assumptions}

\begin{equation}
    \label{eq:ccg:two-stage-problem}
    \min_{x\in X} \ c^\top x + \max_{ u\in U } \ \min_{y\in Y(x,u)} \ d^\top y
\end{equation}

\begin{equation*}
    X \define \Defset{ x\in \tilde{X} }{ Ax \ge a }
\end{equation*}

\begin{equation*}
    Y(x,u) \define \Defset{ y\in \tilde{Y} }{ Cx + Dy + E(x)u \ge b }
\end{equation*}

\begin{equation*}
    U \define \Defset{ u\in\tilde{U} }{ Fu \le g }
\end{equation*}

\begin{assumption}
    \label{assumption:ccg:lower-bounded}
    Problem~\eqref{eq:ccg:two-stage-problem} either has a solution or is
    infeasible. 
\end{assumption}

Note that a sufficient condition for
Assumption~\ref{assumption:ccg:lower-bounded} to hold is that the uncertainty
set $U$ is nonempty and compact and that, for all $u\in U$, the set $\Defset{
(x,y) }{ x\in X, y\in Y(x,u) }$ is compact, possibly empty.

\subsection{The procedure}

At each iteration $K$, the CCG algorithm solves the so-called restricted
master problem which consists in solving
Problem~\eqref{eq:ccg:two-stage-problem} having replaced the uncertainty set
$U$ by a discrete subset of scenarios $\{ u^1, \dotsc, u^K \}$. This problem
can be formulated as  
\begin{subequations}
    \label{eq:ccg:master}
    \begin{align}
        \min_{x\in X, x_0, y^1, \dotsc, y^K\in\tilde{Y}} \quad & c^\top x + x_0 \\
        \text{s.t.} \quad & x_0 \ge d^\top y^k, \quad \text{for all } k=1,\dotsc,K, \\
        & Cx + Dy^k + E(x)u^k \ge b, \quad \text{for all } k=1,\dotsc,K.
    \end{align}
\end{subequations}
Note that vectors $u^k$ with $k=1,\dotsc,K$ are inputs of this model. Also
note that the algorithm requires (at least) one initial scenario $u^1$ so that
the master problem~\eqref{eq:ccg:master} is lower bounded. Several strategies
exist to find this initial scenario and are discussed in
Section~\ref{sec:ccg:initial-scenario}. \HL{discuss unboundedness} Also note
that if, at any iteration $K$, the restricted master
problem~\eqref{eq:ccg:master} is infeasible,
then~\eqref{eq:ccg:two-stage-problem} is infeasible and the algorithm stops.

Now, assume that the restricted master problem~\eqref{eq:ccg:master} is
feasible and let a solution be denoted by $(x,x_0,y^1,\dotsc,y^K)$. We need to
check that the point $x$ is feasilbe for
Problem~\eqref{eq:ccg:two-stage-problem}, i.e., that for all $u\in U$, there
exists $y\in Y(x,u)$, otherwise identify a scenario $u$ for which $Y(x,u)$ is
empty. This is called the ``feasibility separation''. If $x$ is a feasible
first-stage decision, we need to check that the optimal objective value of the
restricted master problem~\eqref{eq:ccg:master} $c^\top x + x_0$ is, indeed,
the correct cost associated to $x$, i.e., that for all $u\in U$, there exists
$y\in Y(x,u)$ such that $d^\top y \le x_0$, otherwise identify a scenario
$u\in U$ with $\min_y\defset{ f^\top y }{ y\in Y(x,u) } > x_0$. This is called
the ``optimality separation''. We now discuss how these two separation tasks
are performed.

In~\textsf{idol}, feasibility separation is done by solving the bilevel
optimization problem
%
\begin{equation}
    \label{eq:ccg:feasibility-separation}
    v_\text{feas} \define \max_{u\in U} \ \min_{y,z} \Defset{ z }{
        y\in\tilde{Y}, \ z\ge 0, \ Cx + Dy + E(x)u + ze \ge b
    }.
\end{equation}
It can be easily checked that if $v_\text{feas} > 0$, then the upper-level
solution $u\in U$ is such that $Y(x,u) = \emptyset$, i.e., $x$ is not a
feasible first-stage decision, and the point $u$ is added to the master
problem~\eqref{eq:ccg:master} for the next iteration. Otherwise, the algorithm
continues with optimality separation (if any). Note that it is not necessary
to solve the separation problen to global optimality to show that
$v_\text{feas}$, any bilevel feasible point would do if its associated
upper-level objective value is strictly greater than 0. Internally,
Problem~\eqref{eq:ccg:feasibility-separation} is always formulated as
%
\begin{subequations}
    \begin{align*}
        - \min_{u\in U,y,z} \quad & -z \\
        \text{s.t.} \quad & (y,z) \in \argmin_{\bar{y},\bar{z}} \Defset{ \bar{z} }{
            \bar{y}\in\tilde{Y}, \ \bar{z}\ge 0, \ Cx + D\bar{y} + E(x)\bar{u} + \bar{z}e \ge b
        }.
    \end{align*}
\end{subequations}
Clearly, the two formulations are equivalent on the level of global solutions.
This formulation allows~\textsf{idol} to rely on any optimizer designed for
general bilevel solver, i.e., solvers implementing
the~\textsf{Bilevel::SolverInterface} interface. Note that this does not
prevent a specific solver to exploit the structure of this specific problem
such as the zero-sum property or the absence of coupling constraints.

Similary, optimality separation is performed by solving the bilevel problem
\begin{equation}
    \label{eq:ccg:optimality-separation}
    v_\text{opt} \define
    \max_{u\in U} \ \min_{y} \Defset{ f^\top y }{
        y\in\tilde{Y}, \ Cx + Dy + E(x)u \ge b
    }.
\end{equation}
Note that this problem is only solved if either feasibility separation has
been turned off---e.g., because the problem is known to have complete
recourse---or the second-stage problem is feasible for all $u\in U$ given $x$.
Hence, Problem~\eqref{eq:ccg:optimality-separation} is always feasible. Note
that $c^\top x + v_\text{opt}$ yields an upper bound on the optimal objective
function value of Problem~\eqref{eq:ccg:two-stage-problem}. This upper bound
is used to show that the algorithm has converged by comparing this upper bound
to the lower bound given, at each iteration, by the master
problem~\eqref{eq:ccg:master}. A scenario $u$ which disproves the optimality
of $x$ is always added to the master problem~\eqref{eq:ccg:master} for the
next iteration. Otherwise, the algorithm stops with a proof of optimality for
$x$. Just like the feasibility separation problem, the optimality separation
problem is internally modeled as the following bilevel problem
%
\begin{subequations}
    \begin{align*}
        - \min_{u\in U,y} \quad & -f^\top y \\
        \text{s.t.} \quad & y \in \argmin_{\bar{y}} \Defset{ f^\top \bar{y} }{
            \bar{y}\in\tilde{Y}, \ Cx + D\bar{y} + E(x)\bar{u} \ge b
        }.
    \end{align*}
\end{subequations}
Again, this allows~\textsf{idol} to rely on general bilevel solvers without
restricting the possibility of exploiting its structure. A complete
description of the procedzre is presented in
Algorithm~\ref{alg:ccg:two-step-separation}.

\begin{algorithm}
    \caption{Column-and-constraint generation with two-step separation}
    \label{alg:ccg:two-step-separation}
    \begin{algorithmic}[1]
        \State ...
        \While {}
        \State ...
        \If{true}
            \State ... 
        \EndIf
        \State ...
        \EndWhile
    \end{algorithmic}
\end{algorithm}

As we discussed it so far, separation is alwyas done in a ``two-step'' manner:
first, feasibility is checked, then optimality is checked. Another possibility
is to perform a single separation which checks both feasibility and optimality
at the same time. We call this separation approach the ``Joint separation''. 

Actually, this
two-step strategy can be replaced by solving a single optimization problem. We
call this approach ``Farkas separation''. Given a current first-stage decision
$x$ and a second-stage cost estimate $x_0$, one solves the bilevel problem 
\begin{equation*}
    v_\text{joint} \define \max_{u\in U} \ \min_{y,z} \ \Defset{ z }{ y\in\tilde{Y}, \ z \ge 0, \ d^\top y - z \le x_0, \ Cx + Dy + E(x)u + ze \ge b }.
\end{equation*}
Let $(u,y,z)$ denote a solution to this bilevel problem---$(y,z)$ denoting an
optimal point of the inner minimization problem given $u$---if $z = 0$, then
the first-stage decision $x$ is both feasible and optimal. Otherwise, one
checks if $y\in Y(x,u)$ holds. If so, then $z = d^\top y - x_0$ and $x_0 +
v_\text{joint}$ is an upper bound on the optimal objective function value of
Problem~\eqref{eq:ccg:two-stage-problem}. Otherwise, $u$ is such that $Y(x,u)
= \emptyset$, which results in a feasibility separation. Joint separation was
introduced by~\textcite{Ayoub2016} for linear problems and was extended to
convex problems by~\textcite{lefebvre2022convex}.
Algorithm~\ref{alg:ccg:joint-separation} presents the overall algorithm.

\begin{algorithm}
    \caption{Column-and-constraint generation with joint separation}
    \label{alg:ccg:joint-separation}
    \begin{algorithmic}[1]
        \State ...
        \While {}
        \State ...
        \If{true}
            \State ... 
        \EndIf
        \State ...
        \EndWhile
    \end{algorithmic}
\end{algorithm}

We have seen that the CCG algorithm requires solving a bilevel optimization
problem at every iteration. In the following two sections, we describe how
these bilevel problems can be solved depending on wether the second-stage
problem is continuous or mixed-integer. 

\subsection{Solving the separation problem with continuous second stage}

In this section, we focus on the specific case in which $\tilde{Y} =
\mathbb{R}^{n_y}$ and discuss various techniques to solve the separation
problem. 

\subsubsection{Optimality separation}

We first consider optimality separation of a current first-stage decision $x$
with second-stage cost estimate $x_0$. We recall that optimality separation is
only performed if $Y(x,u) \neq \emptyset$ for all $u\in U$, i.e., after
feasibility separation or if it is known that $Y(x,u) \neq\emptyset$ for all
$x\in X$ and all $u\in U$. 

\bulletparagraph{KKT-based separation}

Recall that the second-stage primal problem reads
\begin{equation*}
    \min_y\Defset{ f^\top y }{ Cx + Dy + E(x)u \ge b }.
\end{equation*}
Because it is linear, the KKT conditions are both necessary and sufficient for
optimality. Hence, any point $(y,\lambda)$ is a primal-dual solution to the
second-stage problem if and only if it satisfies its associated KKT system
\begin{equation*}
    Cx + Dy + E(x)u \ge b, \quad 
    D^\top \lambda = d, \quad 
    \lambda\ge 0, \quad
    (Cx + Dy + E(x)u - b)^\top \lambda = 0.
\end{equation*}
Thus, one can equivalently replace the lower-level problem in the separation
problem~\eqref{eq:ccg:optimality-separation} by its KKT conditions, leading to
the nonlinear problem
\begin{subequations}
    \begin{align}
        \max_{u,y,\lambda} \quad & d^\top y \\
        \text{s.t.} \quad & u\in U, \\
        & Cx + Dy + E(x)u \ge b, \\ 
        & D^\top \lambda = d, \quad \lambda\ge 0, \\
        & (Cx + Dy + E(x)u - b)^\top \lambda = 0. \label{eq:ccg:kkt}
    \end{align}
\end{subequations}
Note that this problem is a bilinear optimization problem which can be solved
using standard nonlinear techniques. An alternative approach is to linearize
constraints~\eqref{eq:ccg:kkt} by introducing auxiliary binary variables.
However, this approach requires to compute bounds on the dual variables
$\lambda$ which, to the best of our knowledge, cannot be done efficiently
without exploiting problem-specific knowledge. Nevertheless, this additional
work has been shown to be benificial since solving the resulting mixed-integer
linear problem is typically much easier than solving the nonlinear problem.
Thus, assume that we know that there always exists dual solutions satisfying
$\lambda \le M$ for some diagonal matrix $M \ge 0$. Then, the complementarity
constraints~\eqref{eq:ccg:kkt} can be replaced by
\begin{equation*}
    0 \le \lambda \le Mz, \quad 
    0 \le Cx + Dy + E(x)u - b \le M\circ(e - z), \quad 
    z\in\{0,1\}^{m_y}.
\end{equation*}
Replaxing constraints~\eqref{eq:ccg:kkt} by these new constraints lead to a
mixed-integer linear problem. This approach was considered in the seminal
paper~\textcite{Zeng2013} and is a standard approach for solving the
separation problem. One final note. In practice, choosing a wrong value for
$M$ may result in a CCG scheme which is no longer convergent. Hence, it is
necessary that the bounds $M$ be valid. On the contrary, choosing a too large
value for $M$ leads to bad performance in terms of computational time and/or
numerical stability. 

\bulletparagraph{Duality-based separation}

Another approach for solving the separation problem is to exploit strong
duality. Recall that the second-stage problem is feasible for all $u\in U$
given $x$. Hence, the lower-level problem
in~\eqref{eq:ccg:optimality-separation} is feasible and bounded and attains
the same optimal objective function value as its dual problem. The dual reads
\begin{equation*}
    \max_{\lambda} \Defset{ (b - Cx - E(x)u)^\top\lambda }{ D^\top \lambda = d, \ \lambda \ge 0 }.
\end{equation*}
By replacing the lower-level problem by its dual in~\eqref{eq:ccg:optimality-separation} leads to the nonlinear model
\begin{equation}
    \label{eq:ccg:optimality-duality-separation}
    \max_{u\in U, \lambda} \Defset{ (b - Cx - E(x)u)^\top\lambda }{ D^\top \lambda = d, \ \lambda \ge 0 }
\end{equation}
which can be solved by standard nonlinear approaches. 

\bulletparagraph{Duality-KKT-based separation}

Building uppon the duality-based separation, consider
problem~\eqref{eq:ccg:optimality-duality-separation} and let us write it as 
\begin{subequations}
    \begin{align*}
        \max_{\lambda} \quad & (b - Cx)^\top\lambda + 
        \max_{u\in U} \lambda^\top E(x)^\top u \\
        \text{s.t.} \quad & D^\top \lambda = d, \quad \lambda \ge 0.
    \end{align*}
\end{subequations}
Note that the inner maximization problem is feasible and bounded since $U$ is
nonempty and compact. Hence, we may replace it by its KKT conditions
\begin{equation*}
    Fu \le g, \quad \mu \ge 0, \quad F^\top\mu = E(x)^\top\lambda, \quad \mu^\top( Fu - g ) = 0.
\end{equation*}
The resulting formulation for the separation problem is then the nonlinear
problem
\begin{subequations}
    \begin{align*}
        \max_{u\in U,\lambda,\mu} \quad & (b - Cx)^\top\lambda + g^\top\mu \\
        \text{s.t.} \quad & D^\top \lambda = d, \quad \lambda \ge 0, \\
        & \mu \ge 0, \quad F^\top\mu = E(x)^\top\lambda, \quad \mu^\top( Fu - g ) = 0.
    \end{align*}
\end{subequations}

\HLil{ Add comment on linearization.}

\subsubsection{Feasibility separation}

\HLil{todo: add max-min formulation with $\tilde{Y} = \mathbb{R}^{n_y}$}


\begin{equation*}
    Y^{+}(x,u) \define 
    \Defset{
        (y,z)
    }{
        Cx + Dy + E(x)u + ze \ge b, \quad
        z \ge 0
    }.
\end{equation*}

\begin{equation*}
    \max_{u\in U} \quad \min_{(y,s)\in Y^{+}(x,x_0,u)} \ z.
\end{equation*}


\subsubsection*{KKT-based separation}

\HLil{todo}

\subsubsection*{Duality-based separation}

\HLil{todo}

\subsubsection{Avoiding optimality separation}

\HLil{todo: recall max-min formaultion with $\tilde{Y} = \mathbb{R}^{n_y}$}

\begin{equation*}
    Y^{*+}(x,x_0,u) \define 
    \Defset{
        (y,z)
    }{
        x_0 + z \ge d^\top y, \quad
        Cx + Dy + E(x)u + ze \ge b, \quad
        z \ge 0
    }.
\end{equation*}

\begin{equation*}
    \max_{u\in U} \quad \min_{(y,s)\in Y^{*+}(x,x_0,u)} \ z.
\end{equation*}

\subsubsection*{KKT-based separation}

\begin{subequations}
    \begin{align}
        \min_{y,z} \quad & z, \\
        \text{s.t.} \quad & x_0 + z \ge d^\top y, \\
        & Cx + Dy + E(x)u + ze \ge b, \\
        & z \ge 0.
    \end{align}
\end{subequations}

The dual reads 
\begin{subequations}
    \label{eq:ccg:farkas}
    \begin{align}
        \max_{\lambda,\lambda_0,\mu} \quad & -x_0\lambda_0 + (b - Cx - E(x)u)^\top \lambda \\
        \text{s.t.} \quad & D^\top\lambda - d\lambda_0 = 0, \\
        & \lambda_0 + e^\top\lambda + \mu = 1, \\
        & \lambda_0, \lambda, \mu \ge 0.
    \end{align}
\end{subequations}
Complementarity constraints are given as 
\begin{equation*}
    (x_0 + z - d^\top y)\lambda_0 = 0, \quad 
    (Cx + Dy + E(x)u + ze - b)^\top \lambda = 0, \quad 
    z\mu = 0.
\end{equation*}

\subsubsection*{Duality-based separation}

\HLil{todo}

\subsubsection*{Duality-based separation for 0/1 uncertainty sets} 

In the special case where the uncertainty set is either a binary set, i.e., $U
\subseteq \{0,1\}^{n_u}$, or a polytope whose extreme points are all binary,
i.e., $\text{vert}(U) \subseteq \{0,1\}^{n_u}$, \textcite{Ayoub2016} develop
an alternative approach to solve the separation problem. Consider again the
dual problem~\eqref{eq:ccg:farkas}. The authors exploit the fact that
worst-case scenarios are always extreme points of the uncertainty set. Hence,
the separation problem can be modeled as
\begin{subequations}
    \begin{align}
        \max_{u,\lambda,\lambda_0} \quad & -x_0\lambda_0 + (b - Cx - E(x)u)^\top \lambda \\
        \text{s.t.} \quad & D^\top \lambda - d\lambda_0 = 0, \quad \lambda, \lambda_0 \ge 0, \quad \lambda + \lambda_0 \le 1, \label{eq:ccg:farkas-0-1:dual} \\
        & u\in U\cap\{0,1\}^{n_u} \label{eq:ccg:farkas-0-1:uncertainty}.
    \end{align}
\end{subequations}
Then, observe that products between $u_k$ and $\lambda_i$ can be linearized
exactly using the following constraints ensuring $w_{ik} = \lambda_iu_k$
\begin{equation}
    0 \le w_{ik} \le \lambda_i, \quad 
    w_{ik} \le u_k, \quad 
    w_{ik} \ge \lambda_i - 1 + u_k,
    \label{eq:ccg:farkas-0-1:mccormick}
\end{equation}
for all $i=1,\dotsc,m_y$ and all $k = 1,\dotsc,n_u$. Overall, the separation
problem is given by 
\begin{subequations}
    \begin{align*}
        \max_{u,\lambda,\lambda_0,w} \quad & -x_0\lambda_0 + (b - Cx)^\top \lambda - \sum_{i=1}^{m_y} \sum_{k=1}^{n_u} E_{ik}(x)w_{ij} \\
        \text{s.t.} \quad & \text{\eqref{eq:ccg:farkas-0-1:dual}--\eqref{eq:ccg:farkas-0-1:uncertainty}}, \eqref{eq:ccg:farkas-0-1:mccormick}. 
    \end{align*}
\end{subequations}

\subsection{Solving the separation problem with mixed-integer second stage}

We now consider the general case in which $\tilde{Y} =
\mathbb{R}^{p_y}\times\mathbb{Z}^{n_y - p_y}$. 

\HLil{
Approaches are
\begin{itemize}
    \item \textsf{MibS}
    \item CCG Anirudh with big-M and warning with ref to new paper
    \item Farkas and cut generation with binary uncertainty set (new)
\end{itemize}
}

\subsection{Initializing the scenario pool}
\label{sec:ccg:initial-scenario}

\subsubsection{Min and max initialization}

\HLil{todo}

\subsubsection{User scenarios}

\HLil{todo}

\subsubsection{The zero-th iteration}

\HLil{
    Omitting epigraph to get one $x\in X$ and separate.
}

\section[Example: The UFLP with facility disruption]{Example: The uncapacitated facility location problem with facility disruption}

We consider an uncapacitated facility location problem (UFLP) in which
facilities are subject to uncertain disruptions as studied
in~\textcite{Cheng2021}. To that end, let $V_1$ be a set of facilities
location and let $V_2$ be a set customers. For each facility $i\in V_1$, we
let $f_i$ denote the opening cost and $q_i$ its capacity. Each customer~$j\in
V_2$ is associated to a given demand $d_j$ and a marginal penalty for unmet
demand $p_j$. Each connection $(i,j)\in V_1\times V_2$ has a unitary
transportation cost noted $c_{ij}$. The deterministic uncapacitated facility
location problem can be modeled as 
%
\begin{subequations}
    \label{eq:ccg:uflp}
    \begin{align}
        \min_{x,y,z} \quad & \sum_{i\in V_1} f_ix_i + \sum_{i\in V_1} \sum_{j\in V_2} c_{ij} d_j y_{ij} + \sum_{j\in V_2} p_jd_jz_j \\
        \text{s.t.} \quad & \sum_{i\in V_1} y_{ij} + z_j = 1, \quad \text{for all } j\in V_2, \label{eq:ccg:uflp:assignment} \\
        & y_{ij} \le x_i, \quad \text{for all } i\in V_1, \text{for all } j\in V_2,  \label{eq:ccg:uflp:activation} \\
        & y_{ij}\ge 0, z_j \ge 0, \quad \text{for all } i\in V_1, j\in V_2, \label{eq:ccg:uflp:non-negative} \\
        & x_i\in\{0,1\}, \quad \text{for all } i\in V_1.
    \end{align}
\end{subequations}

The uncertain ingredient of the problem under consideration is that some
facilities can be made unavaible. If this is the case, we say that a given
facility is disrupted.
We consider the binary budgeted knapsack uncertainty set 
\begin{equation*}
    U \define \Defset{ u\in\{0,1\}^{|V_1|} }{ \sum_{i\in V_1} u_i \le \Gamma },
\end{equation*}
where, for all facility $i\in V_1$, $u_i = 1$ if and only if faciltiy $i$ is
disrupted. The parameter $\Gamma$ controls the maximum number of facilities
which can be disrupted at the same time and is part of the model. As it
typically occurs in real-world applications, we assume that facilities have to
be planned before any disruption can be anticipated while deciding the
operational decisions of serving customers from facilities can be delayed at a
later instant, where disrupted facilities are known. Hence, the two-stage
robust problem reads 
\begin{equation*}
    \min_{x\in\{0,1\}^{|V_1|}} \Set{
        \sum_{i\in V_1} f_ix_i +
        \max_{u\in U} \ 
        \min_{ y\in Y(x,u) } \ 
        \sum_{i\in V_1} \sum_{j\in V_2} c_{ij} y_{ij}
    },
\end{equation*}
where the second-stage feasible set $Y(x,u)$ is defined for a given
first-stage decision $x\in\{0,1\}^{|V_1|}$ and a given scenario $u\in U$ as
\begin{equation*}
    Y(x,u) \define \Defset{ y\in\mathbb{R}^{|V_1|\times|V_2|} }{
    \text{\eqref{eq:ccg:uflp:assignment}--\eqref{eq:ccg:uflp:non-negative} and }
    y_{ij} \le 1 - u_i, \quad \text{for all }i\in V_1 }.
\end{equation*}

The goal of this example is to show how to implement a CCG algorithm to solve
this problem. To do this, we first need to describe how the adversarial
problem can be solved. This is the subject of the next section.

\subsection{Modeling the robust UFLP with facility disruption in~\textsf{idol}}

As presented in Chapter~\ref{chapter:robust:modeling}, we first need to model
the deterministic model~\eqref{eq:ccg:uflp}. To this end, we will use the
method \textsf{Problems::FLP::read\_instance\_2021\_Cheng\_et\_al(const
std::string\&)} to read an instance file from~\textcite{Cheng2021}. Such
instances can be found
at~\url{https://drive.google.com/drive/folders/1Gy_guJIuLv52ruY89m4Tgrz49FiMspzn?usp=sharing}.
With this, we can read an instance as follows. 
%
\begin{lstlisting}
    const auto instance = Problems::FLP::read_instance_2021_Cheng_et_al("/path/to/instance.txt");
    const unsigned int n_customers = instance.n_customers();
    const unsigned int n_facilities = instance.n_facilities();
\end{lstlisting}

The deterministic model is rather straightforward to model. 
%
\begin{lstlisting}
    Env env;
    Model model(env):

    // Create variables
    const auto x = model.add_vars(Dim<1>(n_facilities), 
                                  0, 1, Binary, 0, "x");
    const auto y = model.add_vars(Dim<2>(n_facilities, n_customers),
                                  0, Inf, Continuous, 0, "y");
    const auto z = model.add_vars(Dim<1>(n_customers),
                                  0, Inf, Continuous, 0, "z");

    // Create assignment constraints
    for (auto j : Range(n_customers)) {
        auto lhs = idol_Sum(i, Range(n_facilities), y[i][j]) + z[j];
        model.add_ctr(lhs <= instance.capacity(i));
    }

    // Create activation constraints
    for (auto i : Range(n_facilities)) {
        for (auto j : Range(n_customers)) {
            model.add_ctr(y[i][j] <= x[i]);
        }
    }

    // Create objective function
    auto objective = 
        idol_Sum(i, Range(n_facilities), 
            instance.fixed_cost(i) * x[i] +
            idol_Sum(j, Range(n_customers),
                instance.per_unit_transportation_cost(i,j) * instance.demand(j) * y[i][j]
            )
        ) + 
        idol_Sum(j, Range(n_customers), 
            instance.per_unit_penalty(j) * instance.demand(j) * z[j]
        );
    model.set_obj_expr(std::move(objective));
\end{lstlisting}

Next, we need to declare the two-stage structure, i.e., describe what variable
and what constraint is part of the second-stage problem. This is done through
the \textsf{Bilevel::Description} class. By default, all variables and
constraints are defined as first-stage variables and constraints. Here, the
second-stage variables are $y$ and $z$ while all constraints are second-stage
constraints. Hence, the following code snippet.
%
\begin{lstlisting}
    Bilevel::Description bilevel_description(env);
    for (auto j : Range(n_customers)) {
        bilevel_description.make_lower_level(z[j]);
        for (auto i : Range(n_facilities)) {
            bilevel_description.make_lower_level(y[i][j]);
        }
    }
\end{lstlisting}
Note that, here, we do not define any coupling constraint.

To end our modeling of the robust UFLP, we need to define two more things: the
uncertainty set and the uncertain coefficients in the second-stage. Let's
start with the uncertainty set. Here, we use $\Gamma = 2$.
%
\begin{lstlisting}
    Model uncertainty_set(env);
    const double Gamma = 2;
    const auto u = uncertainty_set.add_vars(Dim<1>(n_facilities), 
                                            0, 1, Binary, 0, "u");
    uncertainty_set.add_ctr(
        idol_Sum(i, Range(n_facilities), u[i]) <= Gamma
    );
\end{lstlisting}
Finally, we need to describe where this parameter appears in the deterministic
model. We do this through the \textsf{Robust::Description} class. To do this,
we will first need to identify the constraints which are uncertain, i.e., the
activation constraints~\eqref{eq:ccg:uflp:activation}. We can do so, e.g., by
relying on the indices of the constraints within the model we just created.
Indeed, we know that the constraint ``$y_{ij} \le x_i$ '' has an index equal
to $|V_1| + i|V_2| + j$ for all $i\in V_1$ and all $j\in V_2$. Another way
could have been to store these constraints in a separate container or to rely
on the constraints' name. In the uncertain version, the activation
constraint~``$y_{ij} \le x_i$ '' is changed by adding the term ``$- x_i u_i$''
to it. Hence, the following code snippet.
%
\begin{lstlisting}
    Robust::Description robust_description(uncertainty_set);
    for (auto i : Range(n_facilities)) {
        for (auto j : Range(n_customers)) {
            
          // Get activation constraint (i,j)
          const auto index = n_customers + i * n_customers + j;
          const auto& c = model.get_ctr_by_index(index);

          // Add uncertain term
          robust_description.set_uncertain_mat_coeff(c, x[i], u[i]);
        }
    }
\end{lstlisting}
In a nutshell, the call to
\textsf{Robust::Description::set\_uncertain\_mat\_coeff} tells idol that an
uncertain coefficient for $x$ should be added and equals $u_i$, i.e., 
\begin{equation*}
    y_{ij} - x_i \le 0 \quad \longrightarrow \quad y_{ij} - x_i + x_iu_i \le 0.
\end{equation*}

That's it, our robust UFLP is now completely modeled in \textsf{idol}.

\subsection{Preparing the column-and-constraint optimizer}

We are now ready to create our optimizer for solving
problem~\eqref{eq:ccg:uflp}. For CCG, the optimizer has an optimizer factory
called \textsf{Robust::ColumnAndConstraintGeneration} which can be used as
follows. 
%
\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
\end{lstlisting}
As you can see, it is necessary to provide both the bilevel description---so
that~\textsf{idol} knows what variables and constraints are in the first- or
second-stage---, and the robust description---so that uncertain coefficients
as well as the uncertainty set are also known. When we are done configuring
the CCG algorithm, we will be able to call the \textsf{Model::use} method to
set up the optimizer factory and the \textsf{Model::optimize} method to solve
the problem.
%
\begin{lstlisting}
    // Once we are done configuring ccg
    model.use(ccg);
    model.optimize();
\end{lstlisting}
Before we can do so, we need to at least give some information on how to solve
each optimization problems that appear as a sub-problem in the CCG algorithm.
There are essentially two types of problems to solve: the master problem and
the adversarial problem. For the master problem, we will simply
use~\textsf{Gurobi}. We do this through the following code.
%
\begin{lstlisting}
    ccg.with_master_optimizer(Gurobi());
\end{lstlisting}
Then, we need to describe how to solve the adversarial problem, a.k.a., the
separation problem. This is the subject of the next section.

\subsection{Solving the separation problem}

During the CCG algorithm, at every iteration, the master problem is solved. If
the master problem is infeasible, we know that the original two-stage robust
problem is infeasible. Otherwise, let $(x,y^1,\dotsc,y^k)$ for some $k$
corresponding to the number of scenarios present in the master problem. Given
this point, and a current estimate on the second-stage costs $x_0$, we need to
show that either $x$ is a feasible first-stage decision with a second-stage
cost no more than $x_0$, or exhibit a scenario $\hat{u}\in U$ such that either
$Y(x,\hat{u}) = \emptyset$ or the best second-stage decision $y^*$ given $x$
and $\hat{u}$ is such that $d^\top y^* > x_0$.

We describe five different ways to perform this separation exactly and one
heuristic approach along with their implementation in~\textsf{idol}. Note
that, in the case of the UFLP, the second-stage problem is always feasible.
Thus, we ``only'' need to check that $x$ is an optimal first-stage decision. 

\subsubsection{Optimality separation}

\subsubsection*{Duality-based separation}

The first approach is the well-known duality-based approach which consists in
replacing the second-stage primal problem by its dual and linearize products
between dual and uncertain variables in the objective function. Let's see how
it's done. First, since the second-stage problem is always feasible and
bounded, the primal problem attains the same objective value as its dual.
Hence, the primal second-stage problem can be replaced by its dual problem
% 
\begin{subequations}
    \begin{align}
        \max_{\alpha,\beta,\gamma,\delta} \quad & \sum_{ j\in V_2 } \alpha_j + \sum_{i\in V_1} \sum_{j\in V_2} x_i(1 - u_i) \beta_{ij} \\
        \text{s.t.} \quad & \alpha_j + \beta_{ij} + \gamma_{ij} = c_{ij}d_j, \quad \text{for all }i\in V_1, j\in V_2,
        \label{eq:ccg:uflp:dual:y} \\
        & \alpha_j + \delta_j = p_jd_j, \quad\text{for all } j\in V_2, \\
        & \alpha_j\in\mathbb{R}, \beta_{ij} \le 0, \gamma_{ij} \ge 0, \delta_j \ge 0, \quad \text{for all } i\in V_1, j\in V_2.
        \label{eq:ccg:uflp:dual:domains}
    \end{align}
\end{subequations}
Hence, the separation problem reads 
\begin{align*}
    \max_{u,\alpha,\beta,\gamma,\delta} \quad & \sum_{ j\in V_2 } \alpha_j + \sum_{i\in V_1} \sum_{j\in V_2} x_i(1 - u_i)\beta_{ij} \\
    \text{s.t.} \quad & u\in U, \text{\eqref{eq:ccg:uflp:dual:y}--\eqref{eq:ccg:uflp:dual:domains}}.
\end{align*}

To implement this technique in \textsf{idol}, we can use the
\textsf{Bilevel::MinMax::StrongDuality} optimizer factory. It can be
configured as follows.
%
\begin{lstlisting}
    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());

    ccg.add_optimality_separation_optimizer(duality_based);
\end{lstlisting}

By doing so, the optimizer factory \textsf{duality\_based} will be called
every time a new separation problem needs to be solved. Note that, as such, we
did not provide any bounds on the dual variables. Hence, the dualized model
will be solved as a nonlinear problem by~\textsf{Gurobi}; see also
Chapter~\ref{chapter:bilevel:continuous} on bilevel problems with continuous
lower-level problems. However, computational experiments have shown that
linearizing those terms is beneficial whenever possible. Hence, we now show
how such bounds can be passed and exploited by~\textsf{idol}.

It is shown in appendix B.1 of~\textcite{Cheng2021} that a dual solution
always exists with $\beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \} \enifed
M_{ij}$. Thus, we can linearize the products $w_{ij} \define u_i\beta_{ij}$ by
introducing binary variables $v_{ij}$ such that 
\begin{equation}
    M_{ij}v_{ij} \le w_{ij} \le 0, \quad 
    \beta_{ij} \le w_{ij} \le \beta_{ij} - M_{ij}(1 - v_{ij}), \quad 
    v_{ij} \in \{0,1\},
    \label{eq:ccg:uflp:dual:mccormick}
\end{equation}
for all $i\in V_1$ and all $j\in V_2$. Note that~\textsf{idol} can do this
automatically. To use this feature, we simply need to provide these bounds
to~\textsf{idol}. As discussed in Chapter~\ref{chapter:bilevel:continuous},
this can be done by means of a child class of the
\textsf{Reformulators::KKT::BoundProvider} class which will return the
necessary bounds. Here is one possible implementation. Note that the only
bounds which need to be returned in this case are those on $\beta$ since
$\beta$ is the variable involved in a product.
%
\begin{lstlisting}
    class UFLPBoundProvider 
        : public idol::Reformulators::KKT::BoundProvider {
    public:
        // TODO
    };
\end{lstlisting}

The complete code for configuring the CCG algorithm reads as follows. 

\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
    
    ccg.with_master_optimizer(Gurobi());

    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());
    duality_based.with_bound_provider(UFLPBoundProvider());

    ccg.add_optimality_separation_optimizer(duality_based);

    model.use(ccg);
    model.optimize();
\end{lstlisting}

With this code, the dualized model is now being linearized before being solved
by~\textsf{Gurobi}, i.e., it is solved as a mixed-integer linear problem. Note
that there is also a third way to solve the dualized model which is by means
of SOS1 constraints. This can be implemented by using the following code. 
%
\begin{lstlisting}
    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());
    duality_based.with_sos1_constraints();
\end{lstlisting} 

\subsubsection*{KKT-based separation}
Another way to solve the separation problem is to exploit the KKT optimality
conditions of the second-stage problem. These conditions are necessary and
sufficient for a primal-dual point to be optimal for the second-stage primal
and dual problems. These conditions are stated as 
\begin{equation}
    \label{eq:ccg:uflp:kkt}
    \begin{aligned}
        \text{primal feasibility} = & 
        \begin{cases}
            \sum_{i\in V_1} y_{ij} + z_j = 1, & \text{for all } j\in V_2, \\
            y_{ij} \le x_i(1 - u_i), & \text{for all } i\in V_1, \text{for all } j\in V_2,  \\
            y_{ij}\ge 0, z_j \ge 0, & \text{for all } i\in V_1, j\in V_2, \\
        \end{cases} \\
        \text{dual feasibility} = & 
        \begin{cases}
            \alpha_j + \beta_{ij} + \gamma_{ij} = c_{ij}d_j, & \text{for all }i\in V_1, j\in V_2, \\
            \alpha_j + \delta_j = p_jd_j, & \text{for all } j\in V_2, \\
        \end{cases} \\
        \text{stationarity} = &
        \begin{cases}
            \alpha_j\in\mathbb{R}, \beta_{ij} \le 0, \gamma_{ij} \ge 0, \delta_j \ge 0, & \text{for all } i\in V_1, j\in V_2.
        \end{cases} \\
        \text{complementarity} = & 
        \begin{cases}
            \beta_{ij}(y_{ij} - x_i(1 - u_i)) = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2, \\
            \gamma_{ij}y_{ij} = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2, \\
            \delta_jz_j = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2. \\
        \end{cases}
    \end{aligned}
\end{equation}
Hence, the separation problem can be formulated as 
\begin{align*}
    \max_{u,y,\alpha,\beta,\gamma,\delta} \Defset{ d^\top y }{ u\in U, \eqref{eq:ccg:uflp:kkt} }.
\end{align*}

To implement this technique in~\textsf{idol}, we can use the
\textsf{Bilevel::KKT} optimizer factory. It can be configured as follows. 
%
\begin{lstlisting}
    auto kkt = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());

    ccg.add_optimality_separation_optimizer(kkt);
\end{lstlisting}

By doing so, the optimizer factory \textsf{kkt} will be called every time a
new separation problem needs to be solved. Note that, as such, we did not
provide any bounds on the dual variables. Hence, the KKT reformulation will be
solved as a nonlinear problem by~\textsf{Gurobi}. However, it is well-known
that linearizing the complementarity constraints with binary variables yields
much better performance. Hence, we now show how such bounds can be passed and
exploited by~\textsf{idol}.

Recall from the previous section that there always exists a dual solution such
that $\beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \}$. From that, we easily show that
there exists a dual solution satisfying
\begin{align*}
    & 0 \le \alpha_j \le p_jd_j, \quad \text{for all }j\in V_2, \\
    & 0 \ge \beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \}, \quad \text{for all } i\in V_1, \text{for all } j\in V_2, \\
    & 0 \le \gamma_{ij} \le c_{ij}d_j + \max\{ 0, d_j( p_j - c_{ij} ) \}, \quad \text{for all } i\in V_1, \text{for all } j\in V_2, \\
    & 0 \le \delta_j \le p_jd_j, \quad \text{for all } j\in V_2.
\end{align*}
We also have the trivial bounds 
\begin{align*}
    & 0\le y_{ij} \le 1, \quad \text{for all } i\in V_1, j\in V_2, \\
    & 0\le z_j \le 1, \quad \text{for all } j\in V_2, \\
    & 0 \le x_i(1 - u_i) - y_{ij} \le 1 , \quad \text{for all } i\in V_1, \text{for all } j\in V_2.
\end{align*}
With these, the complementarity constraints can be linearized by means of
binary variables. In the following code snippet, we enrich our implementation
of the \textsf{UFLPBoundProvider} class so that it returns bounds for all dual
variables.
%
\begin{lstlisting}
    // TODO ...
\end{lstlisting}

The complete code for configuring the CCG algorithm reads as follows.
\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
    
    ccg.with_master_optimizer(Gurobi());

    auto kkt = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());
    kkt.with_bound_provider(UFLPBoundProvider());

    ccg.add_optimality_separation_optimizer(kkt);

    model.use(ccg);
    model.optimize();
\end{lstlisting}

With this code, the KKT single-level reformulation is now being linearized
before being solved by~\textsf{Gurobi}, i.e., it is solved as a mixed-integer
linear problem. Note that there is also a third way to solve the KKT
reformulation which is by means of SOS1 constraints. This can be implemented
by using the following code.
%
\begin{lstlisting}
    auto ktk = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());
    kkt.with_sos1_constraints();   
\end{lstlisting}

\subsubsection*{Separation by the external bilevel solver \textsf{MibS}}

\HLil{todo}

\subsubsection*{Heuristic separation with PADM}

\HLil{todo}

\subsubsection{Avoiding optimality separation}

\subsubsection*{KKT-based separation}

\HLil{todo}

\subsubsection*{KKT-based separation for 0/1 uncertainty sets}

\HLil{todo}

\subsubsection*{Heuristic separation with PADM}

\HLil{todo}

\section[Example: The CFLP with facility disruption]{Example: The capacitated facility location problem with facility disruption}

\subsection{Problem statement}

We now consider a capacitated variant of the facility location problem studied
in the previous section. This problem can be modeled as
model~\eqref{eq:ccg:uflp} with the following additional constraint:
\begin{equation*}
    \sum_{j\in V_2} d_jy_{ij} \le q_i, \quad \text{for all } i\in V_1.
\end{equation*}

\section{Robust bilevel problems with wait-and-see followers}

\section[Example: The bilevel UFLP with facility location]{Example: The uncapacitated facility location problem with wait-and-see follower and facility disruption}

\subsection{Solving the separation problem}

\subsubsection{Optimality separation}
