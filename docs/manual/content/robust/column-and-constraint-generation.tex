\section{Introduction}

In this chapter, we dive into the column-and-constraint (CCG) algorithm which
is a state-of-the-art method for tackling two-stage robust problems. It was
first introduced by \textcite{Zeng2013} in the context of linear two-stage
problems and was later on extended to broader settings such as mixed-integer
second-stage decisions (see, e.g.,~\cite{zeng2012exact},
\cite{Subramanyam2022} and~\cite{lefebvre2025correctiontolagrangiandual}) or
convex second-stage constraints (see, e.g,~\cite{khademi2022dual}
and~\cite{lefebvre2022convex}).

In a nutshell, the CCG algorithm iteratively constructs and solves a sequence
of restricted master problems which approximates the original problem. To do
this, it considers a finite subset of scenarios in place of the---typically
infinite---original uncertainty set. Then, by solving a separation
problem---the so-called adversarial problem---it identifies a new critical
scenario where the current first-stage solution might fail to satisfy the
second-stage constraints under the full uncertainty set. If such a scenario is
found, it is incorporated into the master problem, and the process repeats.
Otherwise, the algorithm terminates, having found a robust feasible
first-stage decision.

The rest of this chapter is organized as follows...\HL{todo}

\subsection{Problem statement and assumptions}
We consider two-stage robust problems of the form
\begin{equation}
    \label{eq:ccg:two-stage-problem}
    \min_{x\in X} \ c^\top x + \max_{ u\in U } \ \min_{y\in Y(x,u)} \ d^\top y
\end{equation}
where $X$ denotes the set of feasible first-stage decisions, $U$ denotes the
uncertainy set and $Y(x,u)$ denotes the set of feasible second-stage decisions
given a first-stage decision $x\in X$ and a scenario $u\in U$. In what
follows, we consider the fairly general case described by 
\begin{align*}
    X \define & \Defset{ x\in \tilde{X} }{ Ax \ge a }, \\
    U \define & \Defset{ u\in\tilde{U} }{ Fu \le g }, \\
    Y(x,u) \define & \Defset{ y\in \tilde{Y} }{ Cx + Dy + E(x)u \ge b },
\end{align*}
where the sets $\tilde{X}$, $\tilde{U}$ and $\tilde{Y}$ are used to impose
simple restrictions on the first-stage decisions, the uncertain parameters and
the second-stage decisions, respectively. We denote by $n_x$, $n_u$ and $n_y$
the number of first-stage decisions, uncertain parameters and second-stage
decisions, respectively. For instance, with $\tilde{X} = \mathbb{R}^{n_x}$,
$\tilde{U} = \mathbb{R}^{n_u}$ and $\tilde{Y} = \mathbb{R}^{n_y}$,
Problem~\eqref{eq:ccg:two-stage-problem} is a linear two-stage robust problem.

We make the following assumptions which are sufficient to ensure convergence
of the column-and-constraint algorithm. 

\begin{assumption}
    \label{assumption:ccg:compact}
    The sets $X$, $U$ and $Y(x,u)$ are compact, possibly empty, for all~$x\in
    X$ and~\mbox{$u\in U$}.
\end{assumption}

\begin{assumption}
    \label{assumption:ccg:supremum}
    For all $x\in X$, the adversarial problem $\sup_{u\in U} \min_{y\in
    Y(x,u)} d^\top y$ is either infeasible or attains its supremum. 
\end{assumption}

One note about these assumptions. They are sufficient conditions to ensure the
convergence of the CCG algorithm. If they are not satisfied, the CCG algorithm
is not guaranteed to converge and extra care should be taken. Also note that
Assumption~\ref{assumption:ccg:supremum} is \emph{hard} to check at least from
an implementation viewpoint. Thus, \textsf{idol} will not check this
assumption for you. However, the following proposition holds.

\begin{proposition}
    Let Assumption~\ref{assumption:ccg:compact} be satisfied and assume that
    $\tilde{Y} = \mathbb{R}^{n_y}$. Then,
    Assumption~\ref{assumption:ccg:supremum} holds.
\end{proposition}

\subsection{The procedure}

At each iteration $K$, the CCG algorithm solves the so-called restricted
master problem which consists in solving
Problem~\eqref{eq:ccg:two-stage-problem} having replaced the uncertainty set
$U$ by a discrete subset of scenarios $\{ u^1, \dotsc, u^K \}$. This problem
can be formulated as  
\begin{subequations}
    \label{eq:ccg:master}
    \begin{align}
        \min_{x\in X, x_0, y^1, \dotsc, y^K\in\tilde{Y}} \quad & c^\top x + x_0 \\
        \text{s.t.} \quad & x_0 \ge d^\top y^k, \quad \text{for all } k=1,\dotsc,K, \\
        & Cx + Dy^k + E(x)u^k \ge b, \quad \text{for all } k=1,\dotsc,K.
    \end{align}
\end{subequations}
Note that vectors $u^k$ with $k=1,\dotsc,K$ are inputs of this model. Also
note that the algorithm requires (at least) one initial scenario $u^1$ so that
the master problem~\eqref{eq:ccg:master} is lower bounded. Several strategies
exist to find this initial scenario and are discussed in
Section~\ref{sec:ccg:initial-scenario}. Also note that if, at any iteration
$K$, the restricted master problem~\eqref{eq:ccg:master} is infeasible,
then~\eqref{eq:ccg:two-stage-problem} is infeasible and the algorithm stops.

Now, assume that the restricted master problem~\eqref{eq:ccg:master} is
feasible and let a solution be denoted by $(x,x_0,y^1,\dotsc,y^K)$. We need to
check that the point $x$ is feasilbe for
Problem~\eqref{eq:ccg:two-stage-problem}, i.e., that for all $u\in U$, there
exists $y\in Y(x,u)$, otherwise identify a scenario $u$ for which $Y(x,u)$ is
empty. This is called the ``feasibility separation''. If $x$ is a feasible
first-stage decision, we need to check that the optimal objective value of the
restricted master problem~\eqref{eq:ccg:master} $c^\top x + x_0$ is, indeed,
the correct cost associated to $x$, i.e., that for all $u\in U$, there exists
$y\in Y(x,u)$ such that $d^\top y \le x_0$, otherwise identify a scenario
$u\in U$ with $\min_y\defset{ f^\top y }{ y\in Y(x,u) } > x_0$. This is called
the ``optimality separation''. We now discuss how these two separation tasks
are performed.

In~\textsf{idol}, feasibility separation is done by solving the bilevel
optimization problem
%
\begin{equation}
    \label{eq:ccg:feasibility-separation}
    v_\text{feas} \define \max_{u\in U} \ \min_{y,z} \Defset{ z }{
        y\in\tilde{Y}, \ z\ge 0, \ Cx + Dy + E(x)u + ze \ge b
    }.
\end{equation}
It can be easily checked that if $v_\text{feas} > 0$, then the upper-level
solution $u\in U$ is such that $Y(x,u) = \emptyset$, i.e., $x$ is not a
feasible first-stage decision, and the point $u$ is added to the master
problem~\eqref{eq:ccg:master} for the next iteration. Otherwise, the algorithm
continues with optimality separation (if any). Note that it is not necessary
to solve the separation problen to global optimality to show that
$v_\text{feas} > 0$: any bilevel feasible point would do if its associated
upper-level objective value is strictly greater than 0. Internally,
Problem~\eqref{eq:ccg:feasibility-separation} is formulated as
%
\begin{align*}
    - \min_{u\in U,y,z} \quad & -z \\
    \text{s.t.} \quad & (y,z) \in \argmin_{\bar{y},\bar{z}} \Defset{ \bar{z} }{
        \bar{y}\in\tilde{Y}, \ \bar{z}\ge 0, \ Cx + D\bar{y} + E(x)\bar{u} + \bar{z}e \ge b
    }.
\end{align*}
Clearly, the two formulations are equivalent on the level of global solutions.
This formulation allows~\textsf{idol} to rely on any optimizer designed for
general bilevel problems, i.e., solvers implementing
the~\textsf{Bilevel::SolverInterface} interface. Note that this does not
prevent a specific solver to exploit the structure of this specific problem
such as the zero-sum property or the absence of coupling constraints.

Similary, optimality separation is performed by solving the bilevel problem
\begin{equation}
    \label{eq:ccg:optimality-separation}
    v_\text{opt} \define
    \max_{u\in U} \ \min_{y} \Defset{ f^\top y }{
        y\in\tilde{Y}, \ Cx + Dy + E(x)u \ge b
    }.
\end{equation}
Note that this problem is only solved if either feasibility separation has
been turned off---e.g., because the problem is known to have complete
recourse---or the second-stage problem is feasible for all $u\in U$ given $x$.
Hence, Problem~\eqref{eq:ccg:optimality-separation} is always feasible. Note
that $c^\top x + v_\text{opt}$ yields an upper bound on the optimal objective
function value of Problem~\eqref{eq:ccg:two-stage-problem}. This upper bound
is used to show that the algorithm has converged by comparing this upper bound
to the lower bound given, at each iteration, by the master
problem~\eqref{eq:ccg:master}. A scenario $u$ which disproves the optimality
of $x$ is always added to the master problem~\eqref{eq:ccg:master} for the
next iteration. Otherwise, the algorithm stops with a proof of optimality for
$x$. Just like the feasibility separation problem, the optimality separation
problem is internally modeled as the following bilevel problem
%
\begin{align*}
    - \min_{u\in U,y} \quad & -f^\top y \\
    \text{s.t.} \quad & y \in \argmin_{\bar{y}} \Defset{ f^\top \bar{y} }{
        \bar{y}\in\tilde{Y}, \ Cx + D\bar{y} + E(x)\bar{u} \ge b
    }.
\end{align*}
Again, this allows~\textsf{idol} to rely on general bilevel solvers without
restricting the possibility of exploiting its structure. A complete
description of the procedure is presented in
Algorithm~\ref{alg:ccg:two-step-separation}.

\begin{algorithm}
    \caption{Column-and-constraint generation with two-step separation}
    \label{alg:ccg:two-step-separation}
    \begin{algorithmic}[1]
        \State ...
        \While {}
        \State ...
        \If{true}
            \State ... 
        \EndIf
        \State ...
        \EndWhile
    \end{algorithmic}
\end{algorithm}

As we discussed it so far, separation is always done in a ``two-step'' manner:
first, feasibility is checked, then optimality is checked. Another possibility
is to perform a single separation which checks both feasibility and optimality
at the same time. We call this separation approach the ``joint separation''.
Given a current first-stage decision $x$ and a second-stage cost estimate
$x_0$, one solves the bilevel problem 
\begin{equation*}
    v_\text{joint} \define \max_{u\in U} \ \min_{y,z} \ \Defset{ z }{ y\in\tilde{Y}, \ z \ge 0, \ d^\top y - z \le x_0, \ Cx + Dy + E(x)u + ze \ge b }.
\end{equation*}
Let $(u,y,z)$ denote a solution to this bilevel problem---$(y,z)$ denoting an
optimal point of the inner minimization problem given $u$---if $z = 0$, then
the first-stage decision $x$ is both feasible and optimal. Otherwise, one
checks if $y\in Y(x,u)$ holds. If so, then $z = d^\top y - x_0$ and $x_0 +
v_\text{joint}$ is an upper bound on the optimal objective function value of
Problem~\eqref{eq:ccg:two-stage-problem}. Otherwise, $u$ is such that $Y(x,u)
= \emptyset$, which results in a feasibility separation. Joint separation was
introduced by~\textcite{Ayoub2016} for linear problems and was extended to
convex problems by~\textcite{lefebvre2022convex}.
Algorithm~\ref{alg:ccg:joint-separation} presents the overall algorithm.

\begin{algorithm}
    \caption{Column-and-constraint generation with joint separation}
    \label{alg:ccg:joint-separation}
    \begin{algorithmic}[1]
        \State ...
        \While {}
        \State ...
        \If{true}
            \State ... 
        \EndIf
        \State ...
        \EndWhile
    \end{algorithmic}
\end{algorithm}

We have seen that the CCG algorithm requires solving a bilevel optimization
problem at every iteration. In the following two sections, we describe how
these bilevel problems can be solved depending on wether the second-stage
problem is continuous or mixed-integer. 

\subsection{Solving the separation problem with continuous second stage}

In this section, we focus on the specific case in which $\tilde{Y} =
\mathbb{R}^{n_y}$ and discuss various techniques to solve the separation
problem. 

\subsubsection{Optimality separation}

We first consider optimality separation of a current first-stage decision $x$
with second-stage cost estimate $x_0$. We recall that optimality separation is
only performed if $Y(x,u) \neq \emptyset$ for all $u\in U$, i.e., after
feasibility separation or if it is known that $Y(x,u) \neq\emptyset$ for all
$x\in X$ and all $u\in U$. 

\bulletparagraph{KKT-based separation}

Recall that the second-stage primal problem reads
\begin{equation*}
    \min_y\Defset{ f^\top y }{ Cx + Dy + E(x)u \ge b }.
\end{equation*}
Because it is linear, the KKT conditions are both necessary and sufficient for
optimality. Hence, any point $(y,\lambda)$ is a primal-dual solution to the
second-stage problem if and only if it satisfies its associated KKT system
\begin{equation*}
    Cx + Dy + E(x)u \ge b, \quad 
    D^\top \lambda = d, \quad 
    \lambda\ge 0, \quad
    (Cx + Dy + E(x)u - b)^\top \lambda = 0.
\end{equation*}
Thus, one can equivalently replace the lower-level problem in the separation
problem~\eqref{eq:ccg:optimality-separation} by its KKT conditions, leading to
the nonlinear problem
\begin{subequations}
    \begin{align}
        \max_{u,y,\lambda} \quad & d^\top y \\
        \text{s.t.} \quad & u\in U, \\
        & Cx + Dy + E(x)u \ge b, \\ 
        & D^\top \lambda = d, \quad \lambda\ge 0, \\
        & (Cx + Dy + E(x)u - b)^\top \lambda = 0. \label{eq:ccg:kkt}
    \end{align}
\end{subequations}
Note that this problem is a bilinear optimization problem which can be solved
using standard nonlinear techniques. An alternative approach is to linearize
constraints~\eqref{eq:ccg:kkt} by introducing auxiliary binary variables.
However, this approach requires to compute bounds on the dual variables
$\lambda$ which, to the best of our knowledge, cannot be done efficiently
without exploiting problem-specific knowledge. Nevertheless, this additional
work has been shown to be benificial since solving the resulting mixed-integer
linear problem is typically much easier than solving the nonlinear problem.
Thus, assume that we know that there always exists dual solutions satisfying
$\lambda \le M$ for some diagonal matrix $M \ge 0$. Then, the complementarity
constraints~\eqref{eq:ccg:kkt} can be replaced by
\begin{equation*}
    0 \le \lambda \le Mz, \quad 
    0 \le Cx + Dy + E(x)u - b \le M\circ(e - z), \quad 
    z\in\{0,1\}^{m_y}.
\end{equation*}
Replacing constraints~\eqref{eq:ccg:kkt} by these new constraints leads to a
mixed-integer linear problem. This approach was considered in the seminal
paper~\textcite{Zeng2013} and is a standard approach for solving the
separation problem. One final note. In practice, choosing a wrong value for
$M$ may result in a CCG scheme which is no longer convergent. Hence, it is
necessary that the bounds $M$ be valid. On the contrary, choosing a too large
value for $M$ leads to bad performance in terms of computational time and/or
numerical stability. 

\bulletparagraph{Duality-based separation}

Another approach for solving the separation problem is to exploit strong
duality. Recall that the second-stage problem is feasible for all $u\in U$
given $x$. Hence, the lower-level problem
in~\eqref{eq:ccg:optimality-separation} is feasible and bounded and attains
the same optimal objective function value as its dual problem. The dual reads
\begin{equation*}
    \max_{\lambda} \Defset{ (b - Cx - E(x)u)^\top\lambda }{ D^\top \lambda = d, \ \lambda \ge 0 }.
\end{equation*}
Replacing the lower-level problem by its dual
in~\eqref{eq:ccg:optimality-separation} leads to the nonlinear model
\begin{equation}
    \label{eq:ccg:optimality-duality-separation}
    \max_{u\in U, \lambda} \Defset{ (b - Cx - E(x)u)^\top\lambda }{ D^\top \lambda = d, \ \lambda \ge 0 }
\end{equation}
which can be solved by standard nonlinear approaches. 

\bulletparagraph{Duality-KKT-based separation}

Building uppon the duality-based separation, consider
problem~\eqref{eq:ccg:optimality-duality-separation} and let us write it as 
\begin{subequations}
    \begin{align*}
        \max_{\lambda} \quad & (b - Cx)^\top\lambda + 
        \max_{u\in U} -\lambda^\top E(x)^\top u \\
        \text{s.t.} \quad & D^\top \lambda = d, \quad \lambda \ge 0.
    \end{align*}
\end{subequations}
Note that the inner maximization problem is feasible and bounded since $U$ is
nonempty and compact. Hence, we may replace it by its KKT conditions
\begin{equation*}
    Fu \le g, \quad \mu \ge 0, \quad F^\top\mu = -E(x)^\top\lambda, \quad \mu^\top( Fu - g ) = 0.
\end{equation*}
The resulting formulation for the separation problem is then the nonlinear
problem
\begin{subequations}
    \begin{align*}
        \max_{u\in U,\lambda,\mu} \quad & (b - Cx)^\top\lambda + g^\top\mu \\
        \text{s.t.} \quad & D^\top \lambda = d, \quad \lambda \ge 0, \\
        & \mu \ge 0, \quad F^\top\mu = -E(x)^\top\lambda, \quad \mu^\top( Fu - g ) = 0.
    \end{align*}
\end{subequations}

Here again, the nonlinear terms arising from the complementarity constraints
of the KKT conditions can be linearized using auxiliary binary variables and
valid bounds on the dual variables $\mu$. In such a case, the resulting
problem is a mixed-integer linear problem. 

\subsubsection{Feasibility separation}

We now consider solving the feasibility separation problem. The techniques are
similar to those presented in the previous section. Nevertheless, we give a
complete and detailed description of each approach.

\bulletparagraph{KKT-based separation}

Recall the lower-level problem of the separation
problem~\eqref{eq:ccg:feasibility-separation}:
\begin{equation*}
    \min_{y,z} \Defset{ z }{
        Cx + Dy + E(x)u + ze \ge b, \quad 
        z \ge 0
    }.
\end{equation*}
By construction, it is feasible and bounded. Hence, its KKT conditions are
necessary and sufficient to characterize all optimal points. In other words, a
point $(y,\lambda,\mu)$ is a solution to the above problem if and only if it
satisfies the KKT system 
\begin{align*}
    & Cx + Dy + E(x)u + ze \ge b, \quad z \ge 0, \quad 
    D^\top \lambda = 0, \quad \lambda + \mu = 1, \\
    & (Cx + Dy + E(x)u + z - b)^\top\lambda = 0, \quad \mu^\top z = 0.
\end{align*}
Thus, one can equivalently replace the lower-level problem by its KKT
conditions, leading to the nonlinear problem 
\begin{align*}
    \max_{u\in U,y,z,\lambda,\mu} \quad & z \\
    \text{s.t.} \quad 
    & Cx + Dy + E(x)u + ze \ge b, \quad z \ge 0, \quad 
    D^\top \lambda = 0, \quad \lambda + \mu = 1, \\
    & (Cx + Dy + E(x)u + z - b)^\top\lambda = 0, \quad z^\top\mu = 0.
\end{align*}
Again, this problen can be solved ``as is'' using standard nonlinear
approaches. However, introducing auxiliary binary variables to linearize the
complementarity constraints typically leads to better performance. Note that,
contrary to the optimality separation, identifying valid bounds on the dual
variables $\lambda$ and $\mu$ is trivial since $1$ is clearly a valid choice.
Hence, this approach should often be preferred over the nonlinear formulation. 

\bulletparagraph{Duality-based separation}

Replacing the lower-level problem by its dual leads to the duality-based
separation approach. This formulation reads 
\begin{align*}
    \max_{u\in U,\lambda,\mu} \quad & (b - Cx - E(x)u)^\top\lambda \\
    \text{s.t.} \quad
    & D^\top\lambda = 0, \quad \lambda + \mu = 1, \quad \lambda,\mu\ge 0.
\end{align*}
Note that this is a nonlinear problem.

\bulletparagraph{Duality-KKT-based separation}

Building uppon the duality-based separation, we may also write the separation
problem as 
%
\begin{align*}
    \max_{\lambda,\mu} \quad & (b - Cx)^\top\lambda + \max_{u\in U} -\lambda^\top E(x)^\top u \\
    \text{s.t.} \quad & D^\top\lambda = 0, \quad \lambda + \mu = 1, \quad \lambda,\mu\ge 0.
\end{align*}
Replacing the inner problem by its KKT conditions leads to the following
formulation of the separation problem
\begin{align*}
    \max_{u,\lambda,\mu,\nu} \quad & (b - Cx)^\top\lambda + g^\top\nu \\
    \text{s.t.} \quad & D^\top\lambda = 0, \quad \lambda + \mu = 1, \quad \lambda,\mu\ge 0, \\
    & \nu \ge 0, \quad F^\top\nu = -E(x)^\top\lambda, \quad \nu^\top(Fu - g) = 0.
\end{align*}
Again, bounds on the dual variables $\nu$ can be exploited to linearize the
complementarity constraints by introducing auxiliary binary variables.

\subsubsection{Joint separation}

In this section, we consider joint separation, i.e., performing both the
feasibility separation and the optimality separation by solving a single
bilevel problem.


\bulletparagraph{KKT-based separation}

Recall the lower-level problem of the joint separation problem:
%
\begin{subequations}
    \begin{align}
        \min_{y,z} \quad & z \\
        \text{s.t.} \quad & x_0 + z \ge d^\top y, \\
        & Cx + Dy + E(x)u + ze \ge b, \\
        & z \ge 0.
    \end{align}
\end{subequations}
By construction, it is feasible and bounded. Hence, a point
$(y,z,\lambda_0,\lambda,\mu)$ is a primal-dual solution if and only if it
satisfies the KKT system 
\begin{align*}
    & x_0 + z \ge d^\top y, \quad Cx + Dy + E(x)u + ze \ge b, \quad z \ge 0, \\
    & D^\top\lambda - d \lambda_0 = 0, \quad \lambda_0 + e^\top\lambda + \mu = 1, \\
    & (x_0 + z - d^\top y)\lambda_0 = 0, \quad 
    (Cx + Dy + E(x)u + ze - b)^\top \lambda = 0, \quad 
    z\mu = 0.
\end{align*}
Hence, the separation problem can be written as 
\begin{align*}
    \max_{u\in U, y,z,\lambda_0,\lambda,\mu} \quad & z \\
    \text{s.t.} \quad & x_0 + z \ge d^\top y, \quad Cx + Dy + E(x)u + ze \ge b, \quad z \ge 0, \\
    & D^\top\lambda - d \lambda_0 = 0, \quad \lambda_0 + e^\top\lambda + \mu = 1, \\
    & (x_0 + z - d^\top y)\lambda_0 = 0, \quad 
    (Cx + Dy + E(x)u + ze - b)^\top \lambda = 0, \\
    & z\mu = 0.
\end{align*}
%
This is a nonlinear model which can be solved by standard nonlinear
approaches. Moreover, the only nonlinear constraints are the complementarity
constraints which can be linearized by introducing auxiliary binary variables.
Note that this requires bounds on the dual variables $\lambda_0$, $\lambda$
and $\mu$ as well as on the slack variable $z$. Note that the dual variables
are trivially bounded by $1$ while $z$ is typically easy to bound, in
particular if $x$ and $y$ are explicitly bounded. 

\bulletparagraph{Duality-based separation}

Replacing the lower-level problem by its dual leads to the duality-based
separation approach. This formulation reads 
\begin{align*}
    \max_{u\in U,\lambda,\lambda_0,\mu} \quad & -x_0\lambda_0 + (b - Cx - E(x)u)^\top \lambda \\
    \text{s.t.} \quad & D^\top\lambda - d\lambda_0 = 0, \\
    & \lambda_0 + e^\top\lambda + \mu = 1, \\
    & \lambda_0, \lambda, \mu \ge 0.
\end{align*}
Note that this is a nonlinear problem. 

\bulletparagraph{Duality-KKT-based separation}

Building uppon the duality-based approach, we may also write the separation
probem as 
\begin{align*}
    \max_{\lambda,\lambda_0,\mu} \quad & -x_0\lambda_0 + (b - Cx)^\top \lambda + \max_{u\in U}- \lambda^\top E(x)^\top u \\
    \text{s.t.} \quad & D^\top\lambda - d\lambda_0 = 0, \\
    & \lambda_0 + e^\top\lambda + \mu = 1, \\
    & \lambda_0, \lambda, \mu \ge 0.
\end{align*}
Replacing the inner maximization problem by its KKT conditions leads to the
following formulation of the separation problem
\begin{align*}
    \max_{\lambda,\lambda_0,\mu,\nu} \quad & -x_0\lambda_0 + (b - Cx)^\top \lambda + g^\top\nu \\
    \text{s.t.} \quad & D^\top\lambda - d\lambda_0 = 0, \\
    & \lambda_0 + e^\top\lambda + \mu = 1, \\
    & \lambda_0, \lambda, \mu \ge 0, \\
    & \nu\ge 0, \quad F^\top\nu = -E(x)^\top\lambda, \quad \nu^\top(Fu - g) = 0.
\end{align*}
Here again, bounds on $\nu$ can be exploited to linearize the complementarity
constraints and obtain a mixed-integer linear formulation.

\bulletparagraph{Duality-based separation for 0/1 uncertainty sets} 

In the special case where the uncertainty set is either a binary set, i.e., $U
\subseteq \{0,1\}^{n_u}$, or a polytope whose extreme points are all binary,
i.e., $\text{vert}(U) \subseteq \{0,1\}^{n_u}$, \textcite{Ayoub2016} develop
an alternative approach to solve the separation problem. Consider again the
dual of the lower-level problem in the joint separtion problem. The authors
exploit the fact that worst-case scenarios are always extreme points of the
uncertainty set. Hence, the separation problem can be modeled as
\begin{subequations}
    \begin{align}
        \max_{u,\lambda,\lambda_0} \quad & -x_0\lambda_0 + (b - Cx - E(x)u)^\top \lambda \\
        \text{s.t.} \quad & D^\top \lambda - d\lambda_0 = 0, \quad \lambda, \lambda_0 \ge 0, \quad \lambda + \lambda_0 \le 1, \label{eq:ccg:farkas-0-1:dual} \\
        & u\in U\cap\{0,1\}^{n_u} \label{eq:ccg:farkas-0-1:uncertainty}.
    \end{align}
\end{subequations}
Then, observe that products between $u_k$ and $\lambda_i$ can be linearized
exactly using the following constraints ensuring $w_{ik} = \lambda_iu_k$
\begin{equation}
    0 \le w_{ik} \le \lambda_i, \quad 
    w_{ik} \le u_k, \quad 
    w_{ik} \ge \lambda_i - 1 + u_k,
    \label{eq:ccg:farkas-0-1:mccormick}
\end{equation}
for all $i=1,\dotsc,m_y$ and all $k = 1,\dotsc,n_u$. Overall, the separation
problem is given by 
\begin{subequations}
    \begin{align*}
        \max_{u,\lambda,\lambda_0,w} \quad & -x_0\lambda_0 + (b - Cx)^\top \lambda - \sum_{i=1}^{m_y} \sum_{k=1}^{n_u} E_{ik}(x)w_{ij} \\
        \text{s.t.} \quad & \text{\eqref{eq:ccg:farkas-0-1:dual}--\eqref{eq:ccg:farkas-0-1:uncertainty}}, \eqref{eq:ccg:farkas-0-1:mccormick}. 
    \end{align*}
\end{subequations}

\subsection{Solving the separation problem with mixed-integer second stage}

We now consider the general case in which $\tilde{Y} =
\mathbb{R}^{p_y}\times\mathbb{Z}^{n_y - p_y}$. 

\HLil{
Approaches are
\begin{itemize}
    \item \textsf{MibS}
    \item CCG Anirudh with big-M and warning with ref to new paper
    \item Farkas and cut generation with binary uncertainty set (new)
\end{itemize}
}

\renewcommand{\arraystretch}{1.3} % <-- 1.3 = 30% more spacing
\begin{sidewaystable}
    \vspace{12cm}
    \caption{Approaches to solve the separation problem in the CCG algorithm}
    \centering
    \begin{tabular}{lllp{4cm}llp{4cm}}
        \toprule
        & \multicolumn{3}{c}{$\tilde{Y} = \mathbb{R}^{n_y}$} & \multicolumn{3}{c}{$\tilde{Y} = \mathbb{R}^{n_y - p_y}\times\mathbb{Z}^{p_y}$} \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}
        & Approach & Problem type & References & Approach & Problem Type & References \\\toprule
        $\tilde{U}=\mathbb{R}^{n_u}$ & KKT & NLP/MILP & \textcite{Zeng2013} & Nested-CCG & min-max-min & \textcite{zeng2012exact} \\
        & Duality & NLP & \textcite{Zeng2013} \\
        & Duality-KKT & NLP/MILP & \textcite{Ayoub2016}, \textcite{lefebvre2022convex} & & \\[0.5ex]\hdashline{}
        with $\text{vert}(U) \subseteq [0,1]^{n_u}$ & Duality & MILP & \textcite{Ayoub2016}, \textcite{lefebvre2022convex} & --- & --- & --- \\\midrule
        $\tilde{U}=\mathbb{R}^{n_u-p_u}\times\mathbb{Z}^{p_u}$ & KKT & MINLP/MILP & \textcite{Zeng2013} & \textsf{MibS} & Bilevel & \textcite{Tahernejad2020} \\
        & Duality & MINLP & \textcite{Zeng2013} & Nested-CCG & min-max-min & \textcite{zeng2012exact} \\[0.5ex]\hdashline{}
        with only binaries & Duality & MILP & \textcite{Ayoub2016}, \textcite{lefebvre2022convex} & Nested-CGG & min-max-min & \textcite{Subramanyam2022}, \textcite{lefebvre2025correctiontolagrangiandual} \\
        & & & & DW-Duality & MILP & \textcite{pfetsch2021generic} \\\bottomrule 
    \end{tabular}
\end{sidewaystable}
\renewcommand{\arraystretch}{1.0}

\subsection{Initializing the scenario pool}
\label{sec:ccg:initial-scenario}

\subsubsection{Min and max initialization}

\HLil{todo}

\subsubsection{User scenarios}

\HLil{todo}

\subsubsection{The zero-th iteration}

\HLil{
    Omitting epigraph to get one $x\in X$ and separate.
}

\section[Example: the UFLP with disruption]{Example: the uncapacitated facility location problem with disruption}

We consider an uncapacitated facility location problem (UFLP) in which
facilities are subject to uncertain disruptions as studied
in~\textcite{Cheng2021}. To that end, let $V_1$ be a set of facilities
location and let $V_2$ be a set customers. For each facility $i\in V_1$, we
let $f_i$ denote the opening cost and $q_i$ its capacity. Each customer~$j\in
V_2$ is associated to a given demand $d_j$ and a marginal penalty for unmet
demand $p_j$. Each connection $(i,j)\in V_1\times V_2$ has a unitary
transportation cost noted $c_{ij}$. The deterministic uncapacitated facility
location problem can be modeled as 
%
\begin{subequations}
    \label{eq:ccg:uflp}
    \begin{align}
        \min_{x,y,z} \quad & \sum_{i\in V_1} f_ix_i + \sum_{i\in V_1} \sum_{j\in V_2} c_{ij} d_j y_{ij} + \sum_{j\in V_2} p_jd_jz_j \\
        \text{s.t.} \quad & \sum_{i\in V_1} y_{ij} + z_j = 1, \quad \text{for all } j\in V_2, \label{eq:ccg:uflp:assignment} \\
        & y_{ij} \le x_i, \quad \text{for all } i\in V_1, \text{for all } j\in V_2,  \label{eq:ccg:uflp:activation} \\
        & y_{ij}\ge 0, z_j \ge 0, \quad \text{for all } i\in V_1, j\in V_2, \label{eq:ccg:uflp:non-negative} \\
        & x_i\in\{0,1\}, \quad \text{for all } i\in V_1.
    \end{align}
\end{subequations}

The uncertain ingredient of the problem under consideration is that some
facilities can be made unavaible. If this is the case, we say that a given
facility is disrupted.
We consider the binary budgeted knapsack uncertainty set 
\begin{equation*}
    U \define \Defset{ u\in\{0,1\}^{|V_1|} }{ \sum_{i\in V_1} u_i \le \Gamma },
\end{equation*}
where, for all facility $i\in V_1$, $u_i = 1$ if and only if faciltiy $i$ is
disrupted. The parameter $\Gamma$ controls the maximum number of facilities
which can be disrupted at the same time and is part of the model. As it
typically occurs in real-world applications, we assume that facilities have to
be planned before any disruption can be anticipated while deciding the
operational decisions of serving customers from facilities can be delayed at a
later instant, where disrupted facilities are known. Hence, the two-stage
robust problem reads 
\begin{equation*}
    \min_{x\in\{0,1\}^{|V_1|}} \Set{
        \sum_{i\in V_1} f_ix_i +
        \max_{u\in U} \ 
        \min_{ y\in Y(x,u) } \ 
        \sum_{i\in V_1} \sum_{j\in V_2} c_{ij} y_{ij}
    },
\end{equation*}
where the second-stage feasible set $Y(x,u)$ is defined for a given
first-stage decision $x\in\{0,1\}^{|V_1|}$ and a given scenario $u\in U$ as
\begin{equation*}
    Y(x,u) \define \Defset{ y\in\mathbb{R}^{|V_1|\times|V_2|} }{
    \text{\eqref{eq:ccg:uflp:assignment}--\eqref{eq:ccg:uflp:non-negative} and }
    y_{ij} \le 1 - u_i, \quad \text{for all }i\in V_1 }.
\end{equation*}

The goal of this example is to show how to implement a CCG algorithm to solve
this problem. To do this, we first need to describe how the adversarial
problem can be solved. This is the subject of the next section.

\subsection{Modeling the robust UFLP with facility disruption in~\textsf{idol}}

As presented in Chapter~\ref{chapter:robust:modeling}, we first need to model
the deterministic model~\eqref{eq:ccg:uflp}. To this end, we will use the
method \textsf{Problems::FLP::read\_instance\_2021\_Cheng\_et\_al(const
std::string\&)} to read an instance file from~\textcite{Cheng2021}. Such
instances can be found
at~\url{https://drive.google.com/drive/folders/1Gy_guJIuLv52ruY89m4Tgrz49FiMspzn?usp=sharing}.
With this, we can read an instance as follows. 
%
\begin{lstlisting}
    const auto instance = Problems::FLP::read_instance_2021_Cheng_et_al("/path/to/instance.txt");
    const unsigned int n_customers = instance.n_customers();
    const unsigned int n_facilities = instance.n_facilities();
\end{lstlisting}

The deterministic model is rather straightforward to model. 
%
\begin{lstlisting}
    Env env;
    Model model(env):

    // Create variables
    const auto x = model.add_vars(Dim<1>(n_facilities), 
                                  0, 1, Binary, 0, "x");
    const auto y = model.add_vars(Dim<2>(n_facilities, n_customers),
                                  0, Inf, Continuous, 0, "y");
    const auto z = model.add_vars(Dim<1>(n_customers),
                                  0, Inf, Continuous, 0, "z");

    // Create assignment constraints
    for (auto j : Range(n_customers)) {
        auto lhs = idol_Sum(i, Range(n_facilities), y[i][j]) + z[j];
        model.add_ctr(lhs <= instance.capacity(i));
    }

    // Create activation constraints
    for (auto i : Range(n_facilities)) {
        for (auto j : Range(n_customers)) {
            model.add_ctr(y[i][j] <= x[i]);
        }
    }

    // Create objective function
    auto objective = 
        idol_Sum(i, Range(n_facilities), 
            instance.fixed_cost(i) * x[i] +
            idol_Sum(j, Range(n_customers),
                instance.per_unit_transportation_cost(i,j) * instance.demand(j) * y[i][j]
            )
        ) + 
        idol_Sum(j, Range(n_customers), 
            instance.per_unit_penalty(j) * instance.demand(j) * z[j]
        );
    model.set_obj_expr(std::move(objective));
\end{lstlisting}

Next, we need to declare the two-stage structure, i.e., describe what variable
and what constraint is part of the second-stage problem. This is done through
the \textsf{Bilevel::Description} class. By default, all variables and
constraints are defined as first-stage variables and constraints. Here, the
second-stage variables are $y$ and $z$ while all constraints are second-stage
constraints. Hence, the following code snippet.
%
\begin{lstlisting}
    Bilevel::Description bilevel_description(env);
    for (auto j : Range(n_customers)) {
        bilevel_description.make_lower_level(z[j]);
        for (auto i : Range(n_facilities)) {
            bilevel_description.make_lower_level(y[i][j]);
        }
    }
\end{lstlisting}
Note that, here, we do not define any coupling constraint.

To end our modeling of the robust UFLP, we need to define two more things: the
uncertainty set and the uncertain coefficients in the second-stage. Let's
start with the uncertainty set. Here, we use $\Gamma = 2$.
%
\begin{lstlisting}
    Model uncertainty_set(env);
    const double Gamma = 2;
    const auto u = uncertainty_set.add_vars(Dim<1>(n_facilities), 
                                            0, 1, Binary, 0, "u");
    uncertainty_set.add_ctr(
        idol_Sum(i, Range(n_facilities), u[i]) <= Gamma
    );
\end{lstlisting}
Finally, we need to describe where this parameter appears in the deterministic
model. We do this through the \textsf{Robust::Description} class. To do this,
we will first need to identify the constraints which are uncertain, i.e., the
activation constraints~\eqref{eq:ccg:uflp:activation}. We can do so, e.g., by
relying on the indices of the constraints within the model we just created.
Indeed, we know that the constraint ``$y_{ij} \le x_i$ '' has an index equal
to $|V_1| + i|V_2| + j$ for all $i\in V_1$ and all $j\in V_2$. Another way
could have been to store these constraints in a separate container or to rely
on the constraints' name. In the uncertain version, the activation
constraint~``$y_{ij} \le x_i$ '' is changed by adding the term ``$- x_i u_i$''
to it. Hence, the following code snippet.
%
\begin{lstlisting}
    Robust::Description robust_description(uncertainty_set);
    for (auto i : Range(n_facilities)) {
        for (auto j : Range(n_customers)) {
            
          // Get activation constraint (i,j)
          const auto index = n_customers + i * n_customers + j;
          const auto& c = model.get_ctr_by_index(index);

          // Add uncertain term
          robust_description.set_uncertain_mat_coeff(c, x[i], u[i]);
        }
    }
\end{lstlisting}
In a nutshell, the call to
\textsf{Robust::Description::set\_uncertain\_mat\_coeff} tells idol that an
uncertain coefficient for $x$ should be added and equals $u_i$, i.e., 
\begin{equation*}
    y_{ij} - x_i \le 0 \quad \longrightarrow \quad y_{ij} - x_i + x_iu_i \le 0.
\end{equation*}

That's it, our robust UFLP is now completely modeled in \textsf{idol}.

\subsection{Preparing the column-and-constraint optimizer}

We are now ready to create our optimizer for solving
problem~\eqref{eq:ccg:uflp}. For CCG, the optimizer has an optimizer factory
called \textsf{Robust::ColumnAndConstraintGeneration} which can be used as
follows. 
%
\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
\end{lstlisting}
As you can see, it is necessary to provide both the bilevel description---so
that~\textsf{idol} knows what variables and constraints are in the first- or
second-stage---, and the robust description---so that uncertain coefficients
as well as the uncertainty set are also known. When we are done configuring
the CCG algorithm, we will be able to call the \textsf{Model::use} method to
set up the optimizer factory and the \textsf{Model::optimize} method to solve
the problem.
%
\begin{lstlisting}
    // Once we are done configuring ccg
    model.use(ccg);
    model.optimize();
\end{lstlisting}
Before we can do so, we need to at least give some information on how to solve
each optimization problems that appear as a sub-problem in the CCG algorithm.
There are essentially two types of problems to solve: the master problem and
the adversarial problem. For the master problem, we will simply
use~\textsf{Gurobi}. We do this through the following code.
%
\begin{lstlisting}
    ccg.with_master_optimizer(Gurobi());
\end{lstlisting}
Then, we need to describe how to solve the adversarial problem, a.k.a., the
separation problem. This is the subject of the next section.

\subsection{Solving the separation problem}

During the CCG algorithm, at every iteration, the master problem is solved. If
the master problem is infeasible, we know that the original two-stage robust
problem is infeasible. Otherwise, let $(x,y^1,\dotsc,y^k)$ for some $k$
corresponding to the number of scenarios present in the master problem. Given
this point, and a current estimate on the second-stage costs $x_0$, we need to
show that either $x$ is a feasible first-stage decision with a second-stage
cost no more than $x_0$, or exhibit a scenario $\hat{u}\in U$ such that either
$Y(x,\hat{u}) = \emptyset$ or the best second-stage decision $y^*$ given $x$
and $\hat{u}$ is such that $d^\top y^* > x_0$. Note that, in the case of the
UFLP, the second-stage problem is always feasible. Thus, we ``only'' need to
check that $x$ is an optimal first-stage decision. 

\subsubsection{Optimality separation}

In this section, we focus on optimality separation and present various
approaches to implement it with~\textsf{idol}. 

\bulletparagraph{Duality-based separation}

The first approach is the well-known duality-based approach which consists in
replacing the second-stage primal problem by its dual and linearize products
between dual and uncertain variables in the objective function. Let's see how
it's done. First, since the second-stage problem is always feasible and
bounded, the primal problem attains the same objective value as its dual.
Hence, the primal second-stage problem can be replaced by its dual problem
% 
\begin{subequations}
    \begin{align}
        \max_{\alpha,\beta,\gamma,\delta} \quad & \sum_{ j\in V_2 } \alpha_j + \sum_{i\in V_1} \sum_{j\in V_2} x_i(1 - u_i) \beta_{ij} \\
        \text{s.t.} \quad & \alpha_j + \beta_{ij} + \gamma_{ij} = c_{ij}d_j, \quad \text{for all }i\in V_1, j\in V_2,
        \label{eq:ccg:uflp:dual:y} \\
        & \alpha_j + \delta_j = p_jd_j, \quad\text{for all } j\in V_2, \\
        & \alpha_j\in\mathbb{R}, \beta_{ij} \le 0, \gamma_{ij} \ge 0, \delta_j \ge 0, \quad \text{for all } i\in V_1, j\in V_2.
        \label{eq:ccg:uflp:dual:domains}
    \end{align}
\end{subequations}
Hence, the separation problem reads 
\begin{align*}
    \max_{u,\alpha,\beta,\gamma,\delta} \quad & \sum_{ j\in V_2 } \alpha_j + \sum_{i\in V_1} \sum_{j\in V_2} x_i(1 - u_i)\beta_{ij} \\
    \text{s.t.} \quad & u\in U, \text{\eqref{eq:ccg:uflp:dual:y}--\eqref{eq:ccg:uflp:dual:domains}}.
\end{align*}

To implement this technique in \textsf{idol}, we can use the
\textsf{Bilevel::MinMax::StrongDuality} optimizer factory. It can be
configured as follows.
%
\begin{lstlisting}
    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());

    ccg.add_optimality_separation_optimizer(duality_based);
\end{lstlisting}

By doing so, the optimizer factory \textsf{duality\_based} will be called
every time a new separation problem needs to be solved. Note that, as such, we
did not provide any bounds on the dual variables. Hence, the dualized model
will be solved as a nonlinear problem by~\textsf{Gurobi}; see also
Chapter~\ref{chapter:bilevel:continuous} on bilevel problems with continuous
lower-level problems. However, computational experiments have shown that
linearizing those terms is beneficial whenever possible. Hence, we now show
how such bounds can be passed and exploited by~\textsf{idol}.

It is shown in appendix B.1 of~\textcite{Cheng2021} that a dual solution
always exists with $\beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \} \enifed
M_{ij}$. Thus, we can linearize the products $w_{ij} \define u_i\beta_{ij}$ by
introducing binary variables $v_{ij}$ such that 
\begin{equation}
    M_{ij}v_{ij} \le w_{ij} \le 0, \quad 
    \beta_{ij} \le w_{ij} \le \beta_{ij} - M_{ij}(1 - v_{ij}), \quad 
    v_{ij} \in \{0,1\},
    \label{eq:ccg:uflp:dual:mccormick}
\end{equation}
for all $i\in V_1$ and all $j\in V_2$. Note that~\textsf{idol} can do this
automatically. To use this feature, we simply need to provide these bounds
to~\textsf{idol}. As discussed in Chapter~\ref{chapter:bilevel:continuous},
this can be done by means of a child class of the
\textsf{Reformulators::KKT::BoundProvider} class which will return the
necessary bounds. Here is one possible implementation. Note that the only
bounds which need to be returned in this case are those on $\beta$ since
$\beta$ is the variable involved in a product.
%
\begin{lstlisting}
    class UFLPBoundProvider
        : public idol::Reformulators::KKT::BoundProvider {
    public:
        // TODO
    };
\end{lstlisting}

The complete code for configuring the CCG algorithm reads as follows. 

\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
    
    ccg.with_master_optimizer(Gurobi());

    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());
    duality_based.with_bound_provider(UFLPBoundProvider());

    ccg.add_optimality_separation_optimizer(duality_based);

    model.use(ccg);
    model.optimize();
\end{lstlisting}

With this code, the dualized model is now being linearized before being solved
by~\textsf{Gurobi}, i.e., it is solved as a mixed-integer linear problem. Note
that there is also a third way to solve the dualized model which is by means
of SOS1 constraints. This can be implemented by using the following code. 
%
\begin{lstlisting}
    auto duality_based = Bilevel::MinMax::StrongDuality();
    duality_based.with_optimizer(Gurobi());
    duality_based.with_sos1_constraints();
\end{lstlisting} 

\bulletparagraph{KKT-based separation}
Another way to solve the separation problem is to exploit the KKT optimality
conditions of the second-stage problem. These conditions are necessary and
sufficient for a primal-dual point to be optimal for the second-stage primal
and dual problems. These conditions are stated as 
\begin{equation}
    \label{eq:ccg:uflp:kkt}
    \begin{aligned}
        \text{primal feasibility} = & 
        \begin{cases}
            \sum_{i\in V_1} y_{ij} + z_j = 1, & \text{for all } j\in V_2, \\
            y_{ij} \le x_i(1 - u_i), & \text{for all } i\in V_1, \text{for all } j\in V_2,  \\
            y_{ij}\ge 0, z_j \ge 0, & \text{for all } i\in V_1, j\in V_2, \\
        \end{cases} \\
        \text{dual feasibility} = & 
        \begin{cases}
            \alpha_j + \beta_{ij} + \gamma_{ij} = c_{ij}d_j, & \text{for all }i\in V_1, j\in V_2, \\
            \alpha_j + \delta_j = p_jd_j, & \text{for all } j\in V_2, \\
        \end{cases} \\
        \text{stationarity} = &
        \begin{cases}
            \alpha_j\in\mathbb{R}, \beta_{ij} \le 0, \gamma_{ij} \ge 0, \delta_j \ge 0, & \text{for all } i\in V_1, j\in V_2.
        \end{cases} \\
        \text{complementarity} = & 
        \begin{cases}
            \beta_{ij}(y_{ij} - x_i(1 - u_i)) = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2, \\
            \gamma_{ij}y_{ij} = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2, \\
            \delta_jz_j = 0, & \text{for all } i\in V_1, \text{for all } j\in V_2. \\
        \end{cases}
    \end{aligned}
\end{equation}
Hence, the separation problem can be formulated as 
\begin{align*}
    \max_{u,y,\alpha,\beta,\gamma,\delta} \Defset{ d^\top y }{ u\in U, \eqref{eq:ccg:uflp:kkt} }.
\end{align*}

To implement this technique in~\textsf{idol}, we can use the
\textsf{Bilevel::KKT} optimizer factory. It can be configured as follows. 
%
\begin{lstlisting}
    auto kkt = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());

    ccg.add_optimality_separation_optimizer(kkt);
\end{lstlisting}

By doing so, the optimizer factory \textsf{kkt} will be called every time a
new separation problem needs to be solved. Note that, as such, we did not
provide any bounds on the dual variables. Hence, the KKT reformulation will be
solved as a nonlinear problem by~\textsf{Gurobi}. However, it is well-known
that linearizing the complementarity constraints with binary variables yields
much better performance. Hence, we now show how such bounds can be passed and
exploited by~\textsf{idol}.

Recall from the previous section that there always exists a dual solution such
that $\beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \}$. From that, we easily show that
there exists a dual solution satisfying
\begin{align*}
    & 0 \le \alpha_j \le p_jd_j, \quad \text{for all }j\in V_2, \\
    & 0 \ge \beta_{ij} \ge \min\{ 0, d_j(c_{ij} - p_j) \}, \quad \text{for all } i\in V_1, \text{for all } j\in V_2, \\
    & 0 \le \gamma_{ij} \le c_{ij}d_j + \max\{ 0, d_j( p_j - c_{ij} ) \}, \quad \text{for all } i\in V_1, \text{for all } j\in V_2, \\
    & 0 \le \delta_j \le p_jd_j, \quad \text{for all } j\in V_2.
\end{align*}
We also have the trivial bounds 
\begin{align*}
    & 0\le y_{ij} \le 1, \quad \text{for all } i\in V_1, j\in V_2, \\
    & 0\le z_j \le 1, \quad \text{for all } j\in V_2, \\
    & 0 \le x_i(1 - u_i) - y_{ij} \le 1 , \quad \text{for all } i\in V_1, \text{for all } j\in V_2.
\end{align*}
With these, the complementarity constraints can be linearized by means of
binary variables. In the following code snippet, we enrich our implementation
of the \textsf{UFLPBoundProvider} class so that it returns bounds for all dual
variables.
%
\begin{lstlisting}
    // TODO ...
\end{lstlisting}

The complete code for configuring the CCG algorithm reads as follows.
\begin{lstlisting}
    auto ccg = Robust::ColumnAndConstraintGeneration(
                            robust_description,
                            bilevel_description
                        );
    
    ccg.with_master_optimizer(Gurobi());

    auto kkt = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());
    kkt.with_bound_provider(UFLPBoundProvider());

    ccg.add_optimality_separation_optimizer(kkt);

    model.use(ccg);
    model.optimize();
\end{lstlisting}

With this code, the KKT single-level reformulation is now being linearized
before being solved by~\textsf{Gurobi}, i.e., it is solved as a mixed-integer
linear problem. Note that there is also a third way to solve the KKT
reformulation which is by means of SOS1 constraints. This can be implemented
by using the following code.
%
\begin{lstlisting}
    auto ktk = Bilevel::KKT();
    kkt.with_optimizer(Gurobi());
    kkt.with_sos1_constraints();   
\end{lstlisting}

\bulletparagraph{Separation by the external bilevel solver \textsf{MibS}}

\HLil{todo}

\bulletparagraph{Heuristic separation with PADM}

\HLil{todo}

\subsubsection{Joint separation}

\bulletparagraph{KKT-based separation}

\HLil{todo}

\bulletparagraph{KKT-based separation for 0/1 uncertainty sets}

\HLil{todo}

\bulletparagraph{Heuristic separation with PADM}

\HLil{todo}

\section[Example: the CFLP with disruption]{Example: the capacitated facility location problem with disruption}

\subsection{Problem statement}

We now consider a capacitated variant of the facility location problem studied
in the previous section. This problem can be modeled as
model~\eqref{eq:ccg:uflp} with the following additional constraint:
\begin{equation*}
    \sum_{j\in V_2} d_jy_{ij} \le q_i, \quad \text{for all } i\in V_1.
\end{equation*}

\section{Robust bilevel problems with wait-and-see followers}

\section[Example: the bilevel UFLP with disruption]{Example: the uncapacitated facility location problem with disruption and wait-and-see follower}

\subsection{Solving the separation problem}

\subsubsection{Optimality separation}
